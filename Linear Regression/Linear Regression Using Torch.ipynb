{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Using PyTorch Built-ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "tensor_inputs = torch.from_numpy(inputs)\n",
    "tensor_targets = torch.from_numpy(targets)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.],\n        [ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.],\n        [ 73.,  67.,  43.],\n        [ 91.,  88.,  64.],\n        [ 87., 134.,  58.],\n        [102.,  43.,  37.],\n        [ 69.,  96.,  70.]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using `TensorDataset` we will access a small section of the data. The `TensorDataset` retuns a tuple in which first element contains input variables for selected rows and second contains the targets.\n",
    "`DataLoader` will also be used to seperate training data into multiple batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 73.,  67.,  43.],\n         [ 91.,  88.,  64.],\n         [ 87., 134.,  58.]]), tensor([[ 56.,  70.],\n         [ 81., 101.],\n         [119., 133.]]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(tensor_inputs, tensor_targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 69.,  96.,  70.],\n        [ 69.,  96.,  70.],\n        [ 69.,  96.,  70.],\n        [102.,  43.,  37.],\n        [ 91.,  88.,  64.]])\ntensor([[103., 119.],\n        [103., 119.],\n        [103., 119.],\n        [ 22.,  37.],\n        [ 81., 101.]])\n"
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using `nn.Linear` initialize weights and baises automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Parameter containing:\ntensor([[ 0.4222, -0.3010, -0.1563],\n        [-0.0145,  0.0622, -0.3724]], requires_grad=True)\nParameter containing:\ntensor([ 0.5709, -0.0041], requires_grad=True)\n"
    }
   ],
   "source": [
    "model = nn.Linear(3,2) #Here we are giving 3 inputs and expecting 2 outputs so we pass (3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `.parameters` method returns list containing all weights and biases present in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[ 0.4222, -0.3010, -0.1563],\n         [-0.0145,  0.0622, -0.3724]], requires_grad=True),\n Parameter containing:\n tensor([ 0.5709, -0.0041], requires_grad=True)]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `model` direclty for prediction. Also we use built in Loss function instead defining or own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[  4.5070, -12.9110],\n        [  2.5043, -19.6872],\n        [-12.0934, -14.5318],\n        [ 24.9138, -12.5914],\n        [-10.1307, -21.1042],\n        [  4.5070, -12.9110],\n        [  2.5043, -19.6872],\n        [-12.0934, -14.5318],\n        [ 24.9138, -12.5914],\n        [-10.1307, -21.1042],\n        [  4.5070, -12.9110],\n        [  2.5043, -19.6872],\n        [-12.0934, -14.5318],\n        [ 24.9138, -12.5914],\n        [-10.1307, -21.1042]], grad_fn=<AddmmBackward>)\n"
    }
   ],
   "source": [
    "predictions = model(tensor_inputs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.functional` package contains loss functions and several other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(10409.9385, grad_fn=<MseLossBackward>)\n"
    }
   ],
   "source": [
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(model(tensor_inputs),tensor_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use an Optimizer `optim.SGD` Stochastic Gradient Descent to manipulate the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch [10/1000], Loss: 434.1135\nEpoch [20/1000], Loss: 464.6451\nEpoch [30/1000], Loss: 393.4149\nEpoch [40/1000], Loss: 330.8832\nEpoch [50/1000], Loss: 30.3313\nEpoch [60/1000], Loss: 98.9102\nEpoch [70/1000], Loss: 16.9290\nEpoch [80/1000], Loss: 47.8669\nEpoch [90/1000], Loss: 73.8317\nEpoch [100/1000], Loss: 69.6348\nEpoch [110/1000], Loss: 24.5665\nEpoch [120/1000], Loss: 28.8273\nEpoch [130/1000], Loss: 35.0946\nEpoch [140/1000], Loss: 46.0903\nEpoch [150/1000], Loss: 23.6097\nEpoch [160/1000], Loss: 14.1558\nEpoch [170/1000], Loss: 23.4076\nEpoch [180/1000], Loss: 7.9011\nEpoch [190/1000], Loss: 24.7366\nEpoch [200/1000], Loss: 19.3902\nEpoch [210/1000], Loss: 5.6412\nEpoch [220/1000], Loss: 25.0803\nEpoch [230/1000], Loss: 16.0434\nEpoch [240/1000], Loss: 23.2031\nEpoch [250/1000], Loss: 29.3071\nEpoch [260/1000], Loss: 11.3857\nEpoch [270/1000], Loss: 13.9427\nEpoch [280/1000], Loss: 11.7649\nEpoch [290/1000], Loss: 3.1106\nEpoch [300/1000], Loss: 15.1587\nEpoch [310/1000], Loss: 6.1529\nEpoch [320/1000], Loss: 9.7491\nEpoch [330/1000], Loss: 8.7504\nEpoch [340/1000], Loss: 14.6658\nEpoch [350/1000], Loss: 11.6127\nEpoch [360/1000], Loss: 4.9167\nEpoch [370/1000], Loss: 8.1229\nEpoch [380/1000], Loss: 7.6154\nEpoch [390/1000], Loss: 5.3600\nEpoch [400/1000], Loss: 3.7919\nEpoch [410/1000], Loss: 6.0464\nEpoch [420/1000], Loss: 5.7406\nEpoch [430/1000], Loss: 7.3507\nEpoch [440/1000], Loss: 4.9869\nEpoch [450/1000], Loss: 4.9182\nEpoch [460/1000], Loss: 6.2199\nEpoch [470/1000], Loss: 3.7768\nEpoch [480/1000], Loss: 3.5585\nEpoch [490/1000], Loss: 4.3854\nEpoch [500/1000], Loss: 3.6401\nEpoch [510/1000], Loss: 2.9383\nEpoch [520/1000], Loss: 3.3818\nEpoch [530/1000], Loss: 5.5401\nEpoch [540/1000], Loss: 1.9912\nEpoch [550/1000], Loss: 5.9189\nEpoch [560/1000], Loss: 5.1815\nEpoch [570/1000], Loss: 3.9429\nEpoch [580/1000], Loss: 2.4684\nEpoch [590/1000], Loss: 2.6019\nEpoch [600/1000], Loss: 2.3992\nEpoch [610/1000], Loss: 2.5156\nEpoch [620/1000], Loss: 1.2820\nEpoch [630/1000], Loss: 2.1647\nEpoch [640/1000], Loss: 2.9553\nEpoch [650/1000], Loss: 1.6958\nEpoch [660/1000], Loss: 1.8614\nEpoch [670/1000], Loss: 1.0745\nEpoch [680/1000], Loss: 1.7117\nEpoch [690/1000], Loss: 0.7400\nEpoch [700/1000], Loss: 2.6409\nEpoch [710/1000], Loss: 0.9235\nEpoch [720/1000], Loss: 1.9084\nEpoch [730/1000], Loss: 0.8885\nEpoch [740/1000], Loss: 1.5134\nEpoch [750/1000], Loss: 1.9176\nEpoch [760/1000], Loss: 1.4048\nEpoch [770/1000], Loss: 1.1943\nEpoch [780/1000], Loss: 1.6236\nEpoch [790/1000], Loss: 1.3224\nEpoch [800/1000], Loss: 0.8703\nEpoch [810/1000], Loss: 1.3989\nEpoch [820/1000], Loss: 0.8264\nEpoch [830/1000], Loss: 1.0743\nEpoch [840/1000], Loss: 0.6652\nEpoch [850/1000], Loss: 0.9032\nEpoch [860/1000], Loss: 0.9500\nEpoch [870/1000], Loss: 1.2911\nEpoch [880/1000], Loss: 1.2279\nEpoch [890/1000], Loss: 0.9931\nEpoch [900/1000], Loss: 1.1987\nEpoch [910/1000], Loss: 0.8929\nEpoch [920/1000], Loss: 0.8270\nEpoch [930/1000], Loss: 0.7830\nEpoch [940/1000], Loss: 1.0195\nEpoch [950/1000], Loss: 0.5921\nEpoch [960/1000], Loss: 0.5520\nEpoch [970/1000], Loss: 0.7521\nEpoch [980/1000], Loss: 0.7106\nEpoch [990/1000], Loss: 0.7744\nEpoch [1000/1000], Loss: 0.5917\n"
    }
   ],
   "source": [
    "fit(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}